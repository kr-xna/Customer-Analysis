#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Author: kr-xna


Description:
    Data wrangling, analysis, and forecasting of retail transactions
    using PySpark, pandas, ARIMA, and LSTM.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import regexp_replace, col
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_absolute_error
from mlxtend.frequent_patterns import apriori, association_rules
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import itertools


# ========== Utility Functions ==========

def init_spark(app_name: str = "Data Wrangling with PySpark") -> SparkSession:
    return SparkSession.builder.appName(app_name).getOrCreate()

def load_data(spark, file_path: str):
    return spark.read.csv(file_path, header=True, inferSchema=True)


def clean_and_process(df):
    df = df.fillna({'CustomerNo': '-1'})
    df = df.withColumn('ProductName_process',
                        regexp_replace(col('ProductName'), '[^a-zA-Z]', ''))
    return df


def calculate_revenue(df):
    df = df.withColumn('revenue', (col('Price') * col('Quantity')).cast('float'))
    return df


def to_pandas(df):
    pdf = df.toPandas()
    pdf['transaction_date'] = pd.to_datetime(pdf['Date'], format='%m/%d/%Y')
    return pdf


def plot_revenue_over_time(df):
    grouped = df.groupby('transaction_date')['revenue'].sum().reset_index()
    plt.figure(figsize=(10, 6))
    plt.plot(grouped['transaction_date'], grouped['revenue'])
    plt.title('Revenue by Transaction Date')
    plt.xlabel('Transaction Date')
    plt.ylabel('Revenue')
    plt.grid(True)
    plt.show()


def decompose_time_series(df):
    df_full = df.set_index('transaction_date').asfreq('D')
    mean_rev = df_full['revenue'].mean()
    df_full['revenue'].fillna(mean_rev, inplace=True)
    result = seasonal_decompose(df_full['revenue'], model='additive')
    result.plot()
    plt.show()


def arima_grid_search(df, train_end):
    train = df[:train_end]
    test = df[train_end:]
    p = d = q = range(0, 3)
    pdq = list(itertools.product(p, d, q))
    results = []

    for param in pdq:
        try:
            model = ARIMA(train['revenue'], order=param)
            fit = model.fit()
            pred = fit.forecast(steps=len(test))
            pred.index = test.index
            mae = mean_absolute_error(test['revenue'], pred)
            results.append((param, mae))
        except:
            continue

    best_param, best_mae = min(results, key=lambda x: x[1])
    print(f'Best ARIMA params: {best_param} with MAE: {best_mae}')


def lstm_forecast(df):
    df_scaled = MinMaxScaler().fit_transform(df['revenue'].values.reshape(-1, 1))
    train_size = int(len(df_scaled) * 0.8)
    train, test = df_scaled[:train_size], df_scaled[train_size:]

    def create_dataset(data, look_back=30):
        X, Y = [], []
        for i in range(len(data) - look_back):
            X.append(data[i:i+look_back])
            Y.append(data[i+look_back])
        return np.array(X), np.array(Y)

    X_train, y_train = create_dataset(train)
    X_test, y_test = create_dataset(test)

    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

    model = Sequential()
    model.add(LSTM(50, input_shape=(X_train.shape[1], 1)))
    model.add(Dense(1))
    model.compile(loss='mse', optimizer='adam')

    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)

    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    print(f'LSTM MAE: {mae}')


# ========== Main Execution ==========

if __name__ == "__main__":
    spark = init_spark()

    df = load_data(spark, 'transactionrecord.csv')
    df = clean_and_process(df)
    df = calculate_revenue(df)

    df_pandas = to_pandas(df)

    df_pandas[['CustomerNo', 'ProductName', 'ProductName_process', 'transaction_date', 'revenue']].head()

    plot_revenue_over_time(df_pandas)

    decompose_time_series(df_pandas[['transaction_date', 'revenue']])

    arima_grid_search(df_pandas.set_index('transaction_date'), '2019-11-01')

    lstm_forecast(df_pandas.set_index('transaction_date'))

    print("Analysis complete.")
